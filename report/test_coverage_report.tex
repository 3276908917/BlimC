\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}

\usepackage{caption, subcaption, xcolor}

\graphicspath{{res/}}

\title{blimpy Test Coverage}
\author{Lukas Finkbeiner}

\begin{document}

\maketitle

%\begin{abstract}
% I think an abstract would be a bit pompous for such an informal report.
%\end{abstract}

\section{Introduction and Background}

\quad \quad Test coverage is good. We want more. Also documentation. Keep issues alive and open. Also, test centralization.

At various points the tone of this report will shift toward advice, in case some future intern wishes to contribute.

\section{Methods}

\quad \quad I isolate the script that I want to work on.

The blimpy repository includes two scripts necessary for running the complete suite of tests: download\_data.sh and run\_tests.sh. In this repository I have included two pared-down bash scripts, fast.sh, fail.sh. fail.sh is the fastest, but can only be used to check whether new test cases pass. fast.sh is the next fastest, and will check the coverage of all scripts in the blimpy directory. Simpler bash scripts have the advantage of lower overhead, which allows the programmer to easily switch between writing tests and examining their impact. The principle downside of this pared-down approach is that fast.sh ignores repository warnings regarding which scripts are to be tested. Consequently, the programmer will receive coverage results about scripts in the directories `calib\_utils' and `deprecated' (observe the absence of such scripts from the tables below). 

\section{Coverage Results}

\quad \quad The following table records the initial (as of February 2020) state of test coverage. It represents a view similar to what one will receive from running run\_tests.sh in the shell. I have shortened the script names unless the directory is necessary for disambiguation.

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Script & Statements & Misses & Coverage (\%)\\ [0.5ex] 
 \hline
\_\_init\_\_ & 25 & 8 & 68 \\
\hline
bl\_scrunch & 45 & 25 & 44 \\
\hline
dice & 103 & 48 & 53 \\
\hline
ephemeris/\_\_init\_\_ & 3 & 0 & 100 \\
\hline
compute\_lsrk & 28 & 0 & 100 \\
\hline
compute\_lst & 15 & 4 & 73 \\
\hline
ephemeris/config & 9 & 2 & 78 \\
\hline
observatory & 41 & 12 & 71 \\
\hline
fil2h5 & 42 & 21 & 50 \\
\hline
guppi & 271 & 137 & 49 \\
\hline
h52fil & 42 & 21 & 50 \\
\hline
io/\_\_init\_\_ & 2 & 0 & 100 \\
\hline
fil\_writer & 41 & 8 & 80 \\
\hline
file\_wrapper & 397 & 101 & 75 \\
\hline
hdf\_writer & 87 & 20 & 77 \\
\hline
sigproc & 157 & 28 & 82 \\
\hline
match\_fils & 74 & 61 & 18 \\
\hline
plotting/\_\_init\_\_ & 7 & 0 & 100 \\
\hline
plotting/config & 11 & 2 & 82 \\
\hline
plot\_all & 69 & 5 & 93 \\
\hline
plot\_kurtosis & 16 & 2 & 88 \\
\hline
plot\_spectrum & 39 & 6 & 85 \\
\hline
plot\_spectrum\_min\_max & 44 & 5 & 89 \\
\hline
plot\_time\_series & 26 & 3 & 88 \\
\hline
plot\_utils & 28 & 4 & 86 \\
\hline
plot\_waterfall & 28 & 4 & 86 \\
\hline
utils & 65 & 0 & 100 \\
\hline
waterfall & 228 & 57 & 75 \\
\hline
&&&
\\
\hline
TOTAL & 1925 & 581 & 70 \\ [1ex] 
\hline
\end{tabular}

\

In this next table, I report the details of scripts for whom testing has increased. The precise numbers are accurate as of the writing of this report.

 \begin{tabular}{||c c c c||} 
 \hline
 Script & Misses & Decrease & Latest Coverage (\%)\\ [0.5ex] 
\hline
bl\_scrunch & 25 & 12 & 72 \\
\hline
dice & 38 & 10 & 63 \\
\hline
fil2h5 & 8 & 13 & 82 \\
\hline
guppi & 77 & 60 & 72 \\
\hline
h52fil & 8 & 13 & 82 \\
\hline
file\_wrapper & 66* & 17 & 83* \\
\hline
match\_fils & 0* & 10 & 100* \\
\hline
waterfall & 51 & 6 & 78 \\
\hline
&&&
\\
\hline
TOTAL$^\dagger$ & 1931 & 403 & 79 \\ [1ex] 
\hline
\end{tabular}
\end{center}

* asterisk indicates deprecation-assisted numbers
* dagger indicates we are including numbers from unchanged tests

\section{Current Efforts and Obstacles}

\quad \quad The guppi file-handling script guppi.py represented a thorn in the side of this project. Interacting with this script was especially slow. On many occasions, even running tests on this script in isolation, the code-coverage routine would experience a time-out, killing the test session. In visually analyzing the script, I have not come across major areas which could be sped up to resolve this issue. However, virtually all of the user-facing methods (e.g. histogram) deplete the file. As a consequence, each test case must create a new guppi-handler object from scratch. This places a major strain on the hard disk, the slowest piece of a computer.

bl\_scrunch and h52fil.py
    
		* The cmd\_tool does not accept arguments passed into the function (since it is supposed to be run from the command line).
		
    * However, there is one script which allows this: guppi.py

Do we want more data? The test cases will continue to slow down. Diminishing returns,
	but we will be able to weed out if statements every here and there

\section{Concluding Recommendations}

\quad \quad In conclusion, we need some deprecation notices (match\_fils.py). We also need to exclude some methods from our tests, because they raise NotImplementedError (file\_wrapper.py).

Guppi sucks!! I HATE it! Especially because we keep getting killed. File depletion is a major nuisance. And, hard-coding represents a total off-shoot of this project.

We need more documentation all-around. Some methods are fairly self-explanatory, they just run one function and make sure that there are no runtime errors. Other methods, especially those involving command line options, are a bit more involved.

Do we want more rigorous checks for accuracy? The development process will continue to slow down.
Error checking? Not likely, but it is something to think about...

\end{document}
